{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f487e7c3",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression\n",
    "\n",
    "Regression trees are mostly commonly teamed with boosting. There are some additional hyperparameters that need to be set which includes:\n",
    "- number of estimators\n",
    "- learning rate\n",
    "- subsample\n",
    "- max depth\n",
    "\n",
    "We will deal with each of these when it is appropriate. Our goal is to predict the amount of weight loss in cancer patients based on independent variables. This the process we will follow to achieve this.\n",
    "- Data Preperation\n",
    "- Baseline Decision tree model\n",
    "- Hyperparameter tuning\n",
    "- Gradient Boosting model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc23d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from pydataset import data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3439a50",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "The data preparation is not that difficult in this situation. We simply need to load the dataset in an object and remove any missing values. Then we seperate the independent and dependent variables into seperate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849bb41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data(\"cancer\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419cdcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['time', 'sex', 'ph.karno', 'pat.karno', 'meal.cal', 'status']]\n",
    "y = df['wt.loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7a1c9",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "The purpose of the baseline model is to have something to compare our gradient boosting model to. Therefore, all we will do here is to create several regression trees. The difference between the regression trees will be the max depth. The max depth has to with the number of nodes python can make to try to purify the classification. We will then decide which tree is best based on the mean squared error.\n",
    "\n",
    "The first thing we need to do is set the arguments for the cross validation. Cross Validating the results helps to check the accuracy of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe17bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits = 10, shuffle = True, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "488c1447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -193.55304528235052\n",
      "2 -189.2634427676794\n",
      "3 -209.2846723461564\n",
      "4 -218.80238479654003\n",
      "5 -236.7481695179989\n",
      "6 -249.27095314925208\n",
      "7 -294.80522693721264\n",
      "8 -293.8231882876493\n",
      "9 -286.32692707086045\n"
     ]
    }
   ],
   "source": [
    "for depth in range(1,10):\n",
    "    tree_regressor = tree.DecisionTreeRegressor(max_depth = depth, \n",
    "                                               random_state = 1)\n",
    "    if tree_regressor.fit(X, y).tree_.max_depth < depth:\n",
    "        break\n",
    "    score = np.mean(cross_val_score(tree_regressor, X, y,\n",
    "                                   scoring = 'neg_mean_squared_error',\n",
    "                                   cv = crossvalidation, n_jobs = 1))\n",
    "    print(depth, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a497271",
   "metadata": {},
   "source": [
    "You can see thar max depth of 2 had the lowest amount of error. Therefore, our baseline model has a mean squared error of 176. We need to improve on this in order to say that our gradient boosting model is superior\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning has to with setting the value of parameters that the algorithm cannot learn on its own. As such, these are constants that you set as the researcher. The problem is that you are not any better knowing where to set these values than the computer. Therefore, the process that is commonly used is to have the algorithm use several combinations of value until it finds the values that are best for the model. Having said that, there are several hyperparameters we need to tune and there are:\n",
    "- number of estimators\n",
    "- learning rate\n",
    "- subsample\n",
    "- max depth\n",
    "\n",
    "The number of estimators show many trees to create. The more trees the more likely to overfit. The learning rate is the weight tht each tree has on the final prediction. Subsample is the proportion of the sample to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aec6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRB = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2729bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_grid = {\n",
    "    'n_estimators': [500,1000,2000],\n",
    "    'learning_rate': [0.001,0.01,0.1],\n",
    "    'max_depth': [1,2,4],\n",
    "    'subsample': [0.5, 0.75, 1],\n",
    "    'random_state': [1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa04f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(estimator = GRB, param_grid = search_grid,\n",
    "                     scoring = 'neg_mean_squared_error', n_jobs = 1,\n",
    "                     cv = crossvalidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b6aa504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500, 'random_state': 1, 'subsample': 0.5}\n",
      "-160.77818130839782\n"
     ]
    }
   ],
   "source": [
    "search.fit(X, y)\n",
    "print(search.best_params_)\n",
    "print(search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b744173",
   "metadata": {},
   "source": [
    "THe hyperparameter results speaks for themselves. With this tuning we can see that mean squared error is lower than with the baseline model.\n",
    "\n",
    "### Gradient Boosting Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96bea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRB2 = GradientBoostingRegressor(n_estimators = 500, learning_rate = 0.01,\n",
    "                                subsample = 0.5, max_depth = 1, \n",
    "                                random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1af1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.mean(cross_val_score(GRB2, X, y,\n",
    "                                scoring = 'neg_mean_squared_error',\n",
    "                               cv = crossvalidation, n_jobs = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3f41a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-160.77818130839782"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1b9ec",
   "metadata": {},
   "source": [
    "These results were to be exprected. The gradient boosting model has a better performance than the performance than the baseline regression tree model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
